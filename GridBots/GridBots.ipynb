{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUT34eRHc7Ay"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "from random import random as rand\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQmj9PVE-gtg",
        "outputId": "976f137c-a865-4b44-97c1-e354dd65b9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8em_WAG5mLa",
        "outputId": "d893372a-3cc6-442b-dd25-e461d90326d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cpu\n"
          ]
        }
      ],
      "source": [
        "# First define the device for if using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on ' + device.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nCc7bIoc7Az"
      },
      "outputs": [],
      "source": [
        "# Simple helper for initializing grid backdrops\n",
        "def init_a(size):\n",
        "    return np.full((size, size), False, dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALKVjNtSc7Az"
      },
      "outputs": [],
      "source": [
        "# General setup params\n",
        "\n",
        "# Params controlling episode count and length\n",
        "n_episodes = 75000\n",
        "min_iters_per_episode = 40 # We'll setp up the length of the episodes from a minimum number\n",
        "max_iters_per_episode = 75 # ...to a maximum number\n",
        "l_episode_lengths = np.geomspace(min_iters_per_episode\n",
        "                                 , max_iters_per_episode\n",
        "                                 , num=n_episodes).astype(int).tolist() # ...in geometric increments\n",
        "n_sample_every = 2500 # every this many episodes, store a gif and statedict\n",
        "\n",
        "# Params reated to genera environment structure\n",
        "grid_size = 16\n",
        "n_start_bots = 2\n",
        "l_n_targets = np.geomspace(24, 8, num=n_episodes).astype(int).tolist() # This allows us to decay the number of targets but keep it high early\n",
        "n_targets = l_n_targets[0] # for instance, for the first episode, it's the first item in the list\n",
        "vision_size = 4 # number of spaces the bots can see in either direction\n",
        "stun_dist_red = 1 # how far (red or blue's) stun line reaches out\n",
        "stun_dist_blue = 3\n",
        "stun_duration = 4 # how many timesteps a target stays stunned\n",
        "\n",
        "# Params related to rewards and training handling\n",
        "reward_red = False\n",
        "reward_closer = 1 # reward for getting closer to any objective\n",
        "reward_farther = -1 # reward for getting farther from all objective\n",
        "reward_invalid = -1 # reward if move is invalid\n",
        "reward_blue_move_mult = 1 # multiplier for blues movement rewards that allows those rewards to decay\n",
        "blue_mult_decay = 0.99995 # rate of decay for blue's movement reward\n",
        "reward_inaction = 0 # reward for choosing the \"do nothing\" actions\n",
        "reward_target_attempt = 0 # allows a reward to encourage attempting targets\n",
        "reward_target = 1 # reward for securing a target\n",
        "reward_stun_attempt = 0.1 # allows a reward to encourage attempting stunning targets\n",
        "reward_stun = 0 # allows for reward on successful stun\n",
        "reward_stun_miss = -0.1 # penalty for essentially blindly firing stun\n",
        "reward_stun_proximity = 15 # extra reward for getting a stun within a certain range of red\n",
        "stun_proximity_cap = 3 # stun distance near red to count for the proximity reward\n",
        "reward_stun_same = -5 # penalty multiplier if stun is chosen on the same target\n",
        "reward_constant_penalty = -0.1 # constant negative reward always given to disincentivize sitting still\n",
        "entropy_mult_main = -0.001 # how much a lack of entropy in choices negatively affects bots\n",
        "entropy_mult_dir = -0.01 # how much a lack of entropy effects direction choices for main target moves\n",
        "entropy_mult_decay = 0.99995 # fractional decay per episode of entropy_mult_main\n",
        "\n",
        "# Params related to the networks/training\n",
        "l_rate = 0.0002 # learning rate for optimizer\n",
        "gamma = 0.98 # the discount rate for our reward system\n",
        "\n",
        "# Convenience vars (not necessarily customizable, below aligns naturally with code)\n",
        "n_action_types = 3 # only for convenience, must actually remap below if changing\n",
        "n_action_space = 1 + (n_action_types * 8) # 1 \"nothing\" action, plus n action types * 8 directionality options\n",
        "vision_dimz = (vision_size * 2) + 1 # the width and height of the vision arrays (because we see vision_size in both directions)\n",
        "\n",
        "# Storing of team and total scores\n",
        "l_red_raw_scores_episode = [] # these used to track end of episode scores, raw is training scores, objective are for actual goals\n",
        "l_blue_raw_scores_episode = []\n",
        "l_red_moving_raw_score = deque(maxlen=50)\n",
        "l_blue_moving_raw_score = deque(maxlen=50)\n",
        "l_red_moving_obj_score = deque(maxlen=50)\n",
        "l_blue_moving_obj_score = deque(maxlen=50)\n",
        "l_red_obj_scores_episode = []\n",
        "l_blue_obj_scores_episode = []\n",
        "l_red_obj_scores_playback = [] # these used for gif playback\n",
        "l_blue_obj_scores_playback = []\n",
        "l_red_raw_scores_playback = []\n",
        "l_blue_raw_scores_playback = []\n",
        "\n",
        "# Empty lists to store our objects in\n",
        "l_bots_red = []\n",
        "l_bots_blue = []\n",
        "l_targets = []\n",
        "\n",
        "# Arrays to store taken locations in\n",
        "a_bots_red = init_a(grid_size)\n",
        "a_bots_blue = init_a(grid_size)\n",
        "a_targets = init_a(grid_size)\n",
        "a_stuns = init_a(grid_size)\n",
        "a_impass = init_a(grid_size) # for tracking all open positions\n",
        "\n",
        "# Filepaths for recordings\n",
        "fp_a_bots_red = 'a_bots_r.txt'\n",
        "fp_a_bots_blue = 'a_bots_b.txt'\n",
        "fp_a_targets = 'a_targets.txt'\n",
        "fp_a_stuns = 'a_stuns.txt'\n",
        "fp_gif = '/content/drive/MyDrive/Gridbots_Temp/grid_animation'\n",
        "fp_red_brain_statedict = '/content/drive/MyDrive/Gridbots_Temp/red_brain_statedict'\n",
        "fp_blue_brain_statedict = '/content/drive/MyDrive/Gridbots_Temp/blue_brain_statedict'\n",
        "fp_red_optim_statedict = '/content/drive/MyDrive/Gridbots_Temp/red_optim_statedict'\n",
        "fp_blue_optim_statedict = '/content/drive/MyDrive/Gridbots_Temp/blue_optim_statedict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEKctZhjfdyn"
      },
      "outputs": [],
      "source": [
        "# Way tocalculate entropy (useful for penalizing same moves over and over)\n",
        "def calc_entropy(a):\n",
        "    vals, val_counts = np.unique(a, return_counts=True)\n",
        "    p_vals = val_counts / val_counts.sum()\n",
        "    if len(vals) == 1:\n",
        "        e = 0.01\n",
        "    else:\n",
        "        e = entropy(p_vals, base=2)\n",
        "    return e\n",
        "\n",
        "# Scale the entropy calculated above into a loss by taking in an array of choices (scales with multiplier and number of timesteps)\n",
        "def entropy_loss(a, mult=entropy_mult_main):\n",
        "    return mult * len(a) / calc_entropy(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASEJKvtuGbPv"
      },
      "outputs": [],
      "source": [
        "class bot_brain(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(bot_brain, self).__init__()\n",
        "\n",
        "        # General shape params\n",
        "        self.input_chan = 4\n",
        "        self.input_dimz = vision_dimz # the width and height will be the same as the vision dimensions\n",
        "        self.conv_dim = 20 # the \"base\" number of filters to use in the first conv layer\n",
        "        self.output_nodes = n_action_space\n",
        "\n",
        "        # Some misc handling params\n",
        "        self.padding_mode = 'replicate'\n",
        "        self.lrelu_scale = 0.2\n",
        "\n",
        "        # Values for the first conv layer\n",
        "        self.ksize_1 = 3\n",
        "        self.stride_1 = 2\n",
        "        self.padding_1 = 1\n",
        "\n",
        "        # Values for the second conv layer\n",
        "        self.ksize_2 = 3\n",
        "        self.stride_2 = 2\n",
        "        self.padding_2 = 1\n",
        "\n",
        "        # Values for the third conv layer\n",
        "        self.ksize_3 = 3\n",
        "        self.stride_3 = 2\n",
        "        self.padding_3 = 1\n",
        "\n",
        "        # Now our actual conv layers\n",
        "        self.conv1 = nn.Conv2d(self.input_chan, self.conv_dim, self.ksize_1,\n",
        "                               self.stride_1, self.padding_1, padding_mode=self.padding_mode)\n",
        "        self.conv2 = nn.Conv2d(self.conv_dim, self.conv_dim * 2, self.ksize_2,\n",
        "                               self.stride_2, self.padding_2, padding_mode=self.padding_mode)\n",
        "        self.conv3 = nn.Conv2d(self.conv_dim * 2, self.conv_dim * 3, self.ksize_3,\n",
        "                               self.stride_3, self.padding_3, padding_mode=self.padding_mode)\n",
        "        \n",
        "        # Get the number of nodes coming out of the conv network\n",
        "        # = dimensions of last conv layer * number of filters\n",
        "        self.n_conv_output = ((self.input_dimz - 7) ** 2) * (self.conv_dim * 3)\n",
        "        # And the number of nodes for the next layer output\n",
        "        self.n_output_1 = int(self.n_conv_output / 2)\n",
        "\n",
        "        # Now our LSTM and fully connected layers\n",
        "        self.lstm_1 = nn.LSTM(self.n_conv_output\n",
        "                            , self.n_output_1\n",
        "                            , batch_first=True)\n",
        "        self.linear_action = nn.Linear(self.n_output_1, self.output_nodes)\n",
        "        self.linear_critic = nn.Linear(self.n_output_1, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, lstm_1_h, lstm_1_c):\n",
        "\n",
        "      # Run through all of our layers defined above\n",
        "      x = F.leaky_relu(self.conv1(x), self.lrelu_scale)\n",
        "      x = F.leaky_relu(self.conv2(x), self.lrelu_scale)\n",
        "      x = F.leaky_relu(self.conv3(x), self.lrelu_scale)\n",
        "      x = x.view(-1, self.n_conv_output) # flatten the conv layer\n",
        "      # Run through the lstms, don't forget hidden/cell input outputs and to detatch (so as to not keep gradient through all memory)\n",
        "      x, (lstm_1_h_new, lstm_1_c_new) = self.lstm_1(x, (lstm_1_h.detach(), lstm_1_c.detach()))\n",
        "      action_probs = F.softmax(self.linear_action(x), dim=1)\n",
        "      critic_value = self.linear_critic(x)\n",
        "\n",
        "      return action_probs, critic_value, lstm_1_h_new, lstm_1_c_new\n",
        "\n",
        "    def decide_action(self, x, lstm_1_h, lstm_1_c):\n",
        "\n",
        "      # Create the prob distribution from output and return the action/logprob\n",
        "      action_probs, critic_value, lstm_1_h_new, lstm_1_c_new = self.forward(x, lstm_1_h, lstm_1_c)\n",
        "      prob_dist = Categorical(action_probs)\n",
        "      action = prob_dist.sample()\n",
        "      action_item = action.item()\n",
        "      logp = prob_dist.log_prob(action)\n",
        "\n",
        "      return action_item, logp, critic_value, lstm_1_h_new, lstm_1_c_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xHKUwzcn3uj"
      },
      "outputs": [],
      "source": [
        "# Set up our team brains using the above as well as their optimizers\n",
        "team_brain_red = bot_brain().to(device)\n",
        "team_brain_blue = bot_brain().to(device)\n",
        "team_optim_red = optim.Adam(team_brain_red.parameters(), lr=l_rate)\n",
        "team_optim_blue = optim.Adam(team_brain_blue.parameters(), lr=l_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvDTrg8xFgf5"
      },
      "outputs": [],
      "source": [
        "# Load statedicts if we've already done some training\n",
        "# team_brain_red.load_state_dict(torch.load('/content/drive/MyDrive/Gridbots_Temp/first_run/red_brain_statedict_75000.pth'))\n",
        "# team_brain_blue.load_state_dict(torch.load('/content/drive/MyDrive/Gridbots_Temp/first_run/blue_brain_statedict_75000.pth'))\n",
        "# team_brain_red.to(device)\n",
        "# team_brain_blue.to(device)\n",
        "# team_optim_red.load_state_dict(torch.load('/content/drive/MyDrive/Gridbots_Temp/first_run/red_optim_statedict_75000.pth'))\n",
        "# team_optim_blue.load_state_dict(torch.load('/content/drive/MyDrive/Gridbots_Temp/first_run/blue_optim_statedict_75000.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcudhwUdc7A0"
      },
      "outputs": [],
      "source": [
        "class bot():\n",
        "    \n",
        "    def __init__(self, start_x=0, start_y=0, brain=team_brain_red, team='red'):\n",
        "        \n",
        "        # Contextual/positional attributes\n",
        "        self.x_loc = start_x\n",
        "        self.y_loc = start_y\n",
        "        self.team = team\n",
        "\n",
        "        # We need some dimensions based on the team's bot_brain\n",
        "        # These are realistically the same, but we pull as such just in case\n",
        "        if team == 'red':\n",
        "            self.n_output_1 = team_brain_red.n_output_1\n",
        "        else:\n",
        "            self.n_output_1 = team_brain_blue.n_output_1\n",
        "\n",
        "        # Memory based attributes\n",
        "        self.lstm_1_h = torch.zeros(1, self.n_output_1).to(device)\n",
        "        self.lstm_1_c = torch.zeros(1, self.n_output_1).to(device)\n",
        "\n",
        "        # Reward and training related tracking\n",
        "        self.current_ts_reward = 0\n",
        "        self.previous_stun_loc = None\n",
        "        self.l_type_choices = []\n",
        "        self.l_dir_choices = []\n",
        "        self.l_episode_rewards = []\n",
        "        self.l_episode_log_probs = []\n",
        "        self.l_episode_critic_values = []\n",
        "\n",
        "    def reset_memory(self):\n",
        "        self.lstm_1_h = torch.zeros(1, self.n_output_1).to(device)\n",
        "        self.lstm_1_c = torch.zeros(1, self.n_output_1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dDk53Vgc7A1"
      },
      "outputs": [],
      "source": [
        "class target():\n",
        "    \n",
        "    def __init__(self, start_x=0, start_y=0):\n",
        "        \n",
        "        self.x_loc = start_x\n",
        "        self.y_loc = start_y\n",
        "        self.stun_countdown = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fgQKQKIc7A1"
      },
      "outputs": [],
      "source": [
        "# Function to set initial positions and grids of objects\n",
        "def init_positions(object_type, object_list, object_array, n, object_group=None):\n",
        "\n",
        "    # Function will iteratively search for a valid position (checking against what's already taken)\n",
        "    valid_position = False\n",
        "\n",
        "    # Runs for n objects needed to be created\n",
        "    for i in range(n):\n",
        "\n",
        "        while valid_position == False:\n",
        "\n",
        "            # Pick a random x,y pair\n",
        "            start_x = randint(0, grid_size - 1)\n",
        "            start_y = randint(0, grid_size - 1)\n",
        "\n",
        "            # Check if that position is available\n",
        "            if a_impass[start_y, start_x] == False:\n",
        "                \n",
        "                # If so, acknowledge this position will now be taken\n",
        "                valid_position = True\n",
        "                \n",
        "                a_impass[start_y, start_x] = True\n",
        "                object_array[start_y, start_x] = True\n",
        "                \n",
        "                # Append the object to its relevant list\n",
        "                if object_group: # Some objects may have groups, so allow a handling for that\n",
        "                    object_list.append(object_type(start_x=start_x, start_y=start_y, team=object_group))\n",
        "                else:\n",
        "                    object_list.append(object_type(start_x=start_x, start_y=start_y))\n",
        "                \n",
        "        # Reset the while loop for the next run\n",
        "        valid_position = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1buZjyRLc7A1"
      },
      "outputs": [],
      "source": [
        "# Initialize all our positions\n",
        "init_positions(bot, l_bots_red, a_bots_red, n_start_bots, 'red')\n",
        "init_positions(bot, l_bots_blue, a_bots_blue, n_start_bots, 'blue')\n",
        "init_positions(target, l_targets, a_targets, n_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKtqtysFkIiS"
      },
      "outputs": [],
      "source": [
        "# Function used to reset an object type's positions\n",
        "def reset_positions(object_list, object_array):\n",
        "\n",
        "    # Function will iteratively search for a valid position (checking against what's already taken)\n",
        "    valid_position = False\n",
        "\n",
        "    # Runs for n objects needed to be created\n",
        "    for obj in object_list:\n",
        "\n",
        "        # Get the object's current position\n",
        "        x_old, y_old = obj.x_loc, obj.y_loc\n",
        "\n",
        "        while valid_position == False:\n",
        "\n",
        "            # Pick a random x,y pair\n",
        "            x_new = randint(0, grid_size - 1)\n",
        "            y_new = randint(0, grid_size - 1)\n",
        "\n",
        "            # Check if that position is available\n",
        "            if a_impass[y_new, x_new] == False:\n",
        "                \n",
        "                # If so, acknowledge this position will now be taken\n",
        "                valid_position = True\n",
        "                \n",
        "                # Update the arrays for the new positions\n",
        "                a_impass[y_new, x_new] = True\n",
        "                object_array[y_new, x_new] = True\n",
        "\n",
        "                # Update the objects new position\n",
        "                obj.x_loc = x_new\n",
        "                obj.y_loc = y_new\n",
        "                \n",
        "                # Set the old position as empty\n",
        "                a_impass[y_old, x_old] = False\n",
        "                object_array[y_old, x_old] = False          \n",
        "\n",
        "        # Reset the while loop for the next run\n",
        "        valid_position = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLIsBNFpc7A4"
      },
      "outputs": [],
      "source": [
        "# Helpfer functions to saving and loading our numpy arrays to lookback on our work\n",
        "\n",
        "# Store arrays as a text file\n",
        "def save_a(a, fp):\n",
        "    \n",
        "    with open(fp, 'a') as f:\n",
        "        np.savetxt(f, a, delimiter='\\n')\n",
        "\n",
        "# Store all arrays we're interested in\n",
        "def save_all_a():\n",
        "\n",
        "    save_a(a_bots_red, fp_a_bots_red)\n",
        "    save_a(a_bots_blue, fp_a_bots_blue)\n",
        "    save_a(a_targets, fp_a_targets)\n",
        "    save_a(a_stuns, fp_a_stuns)\n",
        "\n",
        "# Load our array, assuming shape is that of the grid\n",
        "def load_grid_a(fp, playback_iters):\n",
        "    \n",
        "    return np.genfromtxt(fp, delimiter='\\n').reshape(playback_iters, grid_size, grid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_eiaa2qc7A0"
      },
      "outputs": [],
      "source": [
        "# For cleaning up text files no longer needed after creating animation\n",
        "def cleanup_files():\n",
        "    os.remove(fp_a_bots_red)\n",
        "    os.remove(fp_a_bots_blue)\n",
        "    os.remove(fp_a_targets)\n",
        "    os.remove(fp_a_stuns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRNj4A3Oc7A4"
      },
      "outputs": [],
      "source": [
        "# Setup for plotting our simulation as a gif\n",
        "\n",
        "# Define some color maps that will be used in our plotting\n",
        "cmap_stun = ListedColormap(['none', 'black'])\n",
        "cmap_red = ListedColormap(['none', 'r'])\n",
        "cmap_blue = ListedColormap(['none', 'b'])\n",
        "cmap_target = ListedColormap(['none', 'g'])\n",
        "\n",
        "# Helper function just for test plots (only handy as a one-off)\n",
        "# def plot_grid(red_positions, blue_positions, target_positions, stun_positions):\n",
        "#     fig, ax = plt.subplots(figsize=(10,10))\n",
        "#     plt.imshow(red_positions, vmin=0, vmax=1, cmap=cmap_red, axes=ax)\n",
        "#     plt.imshow(blue_positions, vmin=0, vmax=1, cmap=cmap_blue, axes=ax)\n",
        "#     plt.imshow(target_positions, vmin=0, vmax=1, cmap=cmap_target, axes=ax)\n",
        "#     plt.imshow(stun_positions, vmin=0, vmax=1, cmap=cmap_stun, axes=ax)\n",
        "#     plt.show()\n",
        "\n",
        "# Function for generating our gif representing the simulation\n",
        "def generate_gif(suffix, playback_iters):\n",
        "\n",
        "    print('Generating gif...')\n",
        "    \n",
        "    # Load our arrays that tracked the sim\n",
        "    record_a_stuns = load_grid_a(fp_a_stuns, playback_iters)\n",
        "    record_a_bots_red = load_grid_a(fp_a_bots_red, playback_iters)\n",
        "    record_a_bots_blue = load_grid_a(fp_a_bots_blue, playback_iters)\n",
        "    record_a_targets = load_grid_a(fp_a_targets, playback_iters)\n",
        "    \n",
        "    # Set up the plot space\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plt.xlim((0, grid_size - 1))\n",
        "    plt.ylim((0, grid_size - 1))\n",
        "    plt.xticks(np.arange(0, grid_size - 1, 2))\n",
        "    plt.yticks(np.arange(0, grid_size - 1, 2))\n",
        "    \n",
        "    # Setup the elements of the plot that will be animated\n",
        "    starter_a = init_a(grid_size) # plt wants an initial data to plot, so we just set some blank array to start\n",
        "    im_bots_stun = plt.imshow(starter_a, vmin=0, vmax=1, cmap=cmap_stun, axes=ax)\n",
        "    im_bots_red = plt.imshow(starter_a, vmin=0, vmax=1, cmap=cmap_red, axes=ax)\n",
        "    im_bots_blue = plt.imshow(starter_a, vmin=0, vmax=1, cmap=cmap_blue, axes=ax)\n",
        "    im_bots_target = plt.imshow(starter_a, vmin=0, vmax=1, cmap=cmap_target, axes=ax)\n",
        "\n",
        "    # Set up score tracking artists\n",
        "    red_text_score = 'Red acquired targets: '\n",
        "    red_score_text = ax.text(\n",
        "      x=0\n",
        "      , y=grid_size - 0.5\n",
        "      , s=red_text_score + ' 0'\n",
        "      , horizontalalignment='left'\n",
        "      , size=10\n",
        "      , color='red'\n",
        "    )\n",
        "    blue_text_score = 'Blue stunned targets: '\n",
        "    blue_score_text = ax.text(\n",
        "      x=grid_size - 1\n",
        "      , y=grid_size - 0.5\n",
        "      , s=blue_text_score + ' 0'\n",
        "      , horizontalalignment='right'\n",
        "      , size=10\n",
        "      , color='blue'\n",
        "    )\n",
        "    \n",
        "    # We need a function to handle animating (on the ith element of our overall data)\n",
        "    def animate(i):\n",
        "        \n",
        "        # Change each plot element to the ith data element\n",
        "        im_bots_stun.set_data(record_a_stuns[i])\n",
        "        im_bots_red.set_data(record_a_bots_red[i])\n",
        "        im_bots_blue.set_data(record_a_bots_blue[i])\n",
        "        im_bots_target.set_data(record_a_targets[i])\n",
        "        red_text = red_text_score + str(l_red_obj_scores_playback[i]) + ' | raw score: ' + str(round(l_red_raw_scores_playback[i], 2))\n",
        "        red_score_text.set_text(red_text)\n",
        "        blue_text = blue_text_score + str(l_blue_obj_scores_playback[i]) + ' | raw score: ' + str(round(l_blue_raw_scores_playback[i], 2))\n",
        "        blue_score_text.set_text(blue_text)\n",
        "        \n",
        "        # Return the above (but must be in the form of a list)\n",
        "        return [im_bots_stun], [im_bots_red], [im_bots_blue], [im_bots_target], [red_score_text], [blue_score_text]\n",
        "    \n",
        "    # Run our animation\n",
        "    grid_animator = animation.FuncAnimation(fig, func=animate, frames=playback_iters, interval=100, repeat_delay=5000)\n",
        "    # Write it to a file\n",
        "    pillow = animation.PillowWriter(fps=1)\n",
        "    grid_animator.save(fp_gif + f'_{suffix}.gif', writer=pillow)\n",
        "\n",
        "    # Cleanup old array text files and any lists\n",
        "    del l_red_obj_scores_playback[:]\n",
        "    del l_blue_obj_scores_playback[:]\n",
        "    del l_red_raw_scores_playback[:]\n",
        "    del l_blue_raw_scores_playback[:]\n",
        "    cleanup_files()\n",
        "\n",
        "    # Close out all plt windows so they don't build up over the loops\n",
        "    plt.close('all')\n",
        "    \n",
        "    print('Done building gif.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v54HGBjIc7A2"
      },
      "outputs": [],
      "source": [
        "# Helper functions for mapping action indices to directions and types\n",
        "\n",
        "# Creates a list separated by 8 steps (8 directions) given an index base\n",
        "# List length is determined by total number of action types\n",
        "def create_direction_set(index_base):\n",
        "    \n",
        "    return [(index_base + (8 * i)) for i in range(n_action_types)]\n",
        "\n",
        "# Define directionality sets (list of action indices for a given direction)\n",
        "l_directions_u = create_direction_set(1)\n",
        "l_directions_ur = create_direction_set(2) # i.e. up right\n",
        "l_directions_r = create_direction_set(3)\n",
        "l_directions_dr = create_direction_set(4)\n",
        "l_directions_d = create_direction_set(5)\n",
        "l_directions_dl = create_direction_set(6) # i.e. down left\n",
        "l_directions_l = create_direction_set(7)\n",
        "l_directions_ul = create_direction_set(8)\n",
        "\n",
        "# Define some action type sets\n",
        "l_action_set_1 = np.arange(1, (8 * 1) + 1)\n",
        "l_action_set_2 = np.arange((8 * 1) + 1, (8 * 2) + 1)\n",
        "l_action_set_3 = np.arange((8 * 2) + 1, (8 * 3) + 1)\n",
        "\n",
        "# Maps an action index to a directionality (and returns the x,y offset)\n",
        "def map_action_direction(action_index):\n",
        "    \n",
        "    if action_index == 0:\n",
        "        x_adjust = 0\n",
        "        y_adjust = 0\n",
        "    elif action_index in (l_directions_u):\n",
        "        x_adjust = 0\n",
        "        y_adjust = 1\n",
        "    elif action_index in (l_directions_ur):\n",
        "        x_adjust = 1\n",
        "        y_adjust = 1\n",
        "    elif action_index in (l_directions_r):\n",
        "        x_adjust = 1\n",
        "        y_adjust = 0\n",
        "    elif action_index in (l_directions_dr):\n",
        "        x_adjust = 1\n",
        "        y_adjust = -1\n",
        "    elif action_index in (l_directions_d):\n",
        "        x_adjust = 0\n",
        "        y_adjust = -1\n",
        "    elif action_index in (l_directions_dl):\n",
        "        x_adjust = -1\n",
        "        y_adjust = -1\n",
        "    elif action_index in (l_directions_l):\n",
        "        x_adjust = -1\n",
        "        y_adjust = 0\n",
        "    elif action_index in (l_directions_ul):\n",
        "        x_adjust = -1\n",
        "        y_adjust = 1\n",
        "    else:\n",
        "        x_adjust = None\n",
        "        y_adjust = None\n",
        "\n",
        "    return (x_adjust, y_adjust)\n",
        "\n",
        "# Maps an action index to an action type\n",
        "def map_action_type(action_index):\n",
        "    \n",
        "    if action_index == 0:\n",
        "        action_type = 'none'\n",
        "    elif action_index in l_action_set_1:\n",
        "        action_type = 'move'\n",
        "    elif action_index in l_action_set_2:\n",
        "        action_type = 'target'\n",
        "    elif action_index in l_action_set_3:\n",
        "        action_type = 'stun'\n",
        "    else:\n",
        "        action_type = None # in which case it's an actual error/bug\n",
        "        \n",
        "    return action_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ZAwGHGc7A2"
      },
      "outputs": [],
      "source": [
        "# Helper function to make sure an x,y isn't off the grid\n",
        "def is_valid_loc(x, y):\n",
        "    \n",
        "    x_is_valid = (0 <= x < grid_size)\n",
        "    y_is_valid = (0 <= y < grid_size)\n",
        "    loc_is_valid = x_is_valid and y_is_valid\n",
        "    \n",
        "    return loc_is_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPiop7zCj3Tk"
      },
      "outputs": [],
      "source": [
        "# Function for finding a \"vision\" array giving an x,y center coord and the array\n",
        "# Out-of-bounds are padded with -1s\n",
        "def vision_a(a, x, y, vision_limit=vision_size):\n",
        "\n",
        "  # Convert the (boolean) array to int\n",
        "  a_int = a.astype(int)\n",
        "  # Get the x,y ranges based on the vision size\n",
        "  x_left, x_right = x - vision_limit, x + vision_limit\n",
        "  y_down, y_up = y + vision_limit, y - vision_limit\n",
        "  # Pad the array to handle out of bounds\n",
        "  padded_array = np.pad(a_int, vision_limit, mode='constant', constant_values=-1)\n",
        "  # Slice the padded array\n",
        "  # Note we need to account for added vision size to the index, that \"up\" are lower values, and the effect of slicing\n",
        "  vision_array = padded_array[\n",
        "        (y_up + vision_limit):(y_down + vision_limit + 1)\n",
        "        , (x_left + vision_limit):(x_right + vision_limit + 1)\n",
        "        ]\n",
        "\n",
        "  return vision_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5mcxcDZ2xTX"
      },
      "outputs": [],
      "source": [
        "# Function to prepare vision input to bot's network given its location\n",
        "def prep_vision(x, y):\n",
        "\n",
        "    # Create an individual vision array for all the relevant inputs\n",
        "    l_input_arrays = [\n",
        "      a_bots_red\n",
        "      , a_bots_blue\n",
        "      , a_targets\n",
        "      , a_impass \n",
        "      ]\n",
        "    l_vision_arrays = [vision_a(a, x, y) for a in l_input_arrays]\n",
        "\n",
        "    # Stack the arrays together\n",
        "    a_vision_all = np.stack(l_vision_arrays)\n",
        "\n",
        "    # Create the tensor version of the array\n",
        "    # Reshape it so that it's batch dim (only 1) x n chans x width x height\n",
        "    t_vision_all = torch.FloatTensor(a_vision_all)\\\n",
        "      .view(-1, len(l_input_arrays), vision_dimz, vision_dimz)\n",
        "\n",
        "    return t_vision_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0vYftfL-XJ1"
      },
      "outputs": [],
      "source": [
        "# Helper functions related to seeing if movement brought a bot closer to a target\n",
        "\n",
        "# Function for simple euclidean distance\n",
        "def euclidean_dist(x1, y1, x2, y2):\n",
        "\n",
        "  x_dist = (x2 - x1) ** 2\n",
        "  y_dist = (y2 - y1) ** 2\n",
        "  dist = (x_dist + y_dist) ** 0.5\n",
        "\n",
        "  return dist\n",
        "\n",
        "# Function that returns the shortest distance to an object (from a given array)\n",
        "# Input is the x,y coord of the bot\n",
        "def shortest_object_dist(x1, y1, a):\n",
        "\n",
        "  # Array of idxs where a target exists\n",
        "  target_coords = np.argwhere(a == True)\n",
        "\n",
        "  # Loop through all coords and append distances to a list\n",
        "  distances = []\n",
        "  for coord in target_coords:\n",
        "    y2 = coord[0]\n",
        "    x2 = coord[1]\n",
        "    distances.append(euclidean_dist(x1, y1, x2, y2))\n",
        "\n",
        "  # Return the shortest distance\n",
        "  return min(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTh5fWPq87Xf"
      },
      "outputs": [],
      "source": [
        "# Setup for moving targets on each timestep away from bots\n",
        "\n",
        "# Convenience list of possible directions a target could move in\n",
        "l_possible_adjustments = [\n",
        "    (0,0) # stay\n",
        "    , (0,1) # up\n",
        "    , (0,-1) # down\n",
        "    , (1,0) # right\n",
        "    , (1,1) # up right\n",
        "    , (1,-1) # down right\n",
        "    , (-1,0) # left\n",
        "    , (-1,1) # up left\n",
        "    , (-1,-1) # down left\n",
        "]\n",
        "\n",
        "# Function that, given a target's current location, return the location that moves it farthers from all bots\n",
        "def get_best_target_loc(x, y):\n",
        "\n",
        "    # Step through some lists to determine possible moves it could make\n",
        "    l_possible_new_locs = [(x + adj[0], y + adj[1]) for adj in l_possible_adjustments] # list of new locs given all directions\n",
        "    l_valid_new_locs = [loc for loc in l_possible_new_locs if is_valid_loc(loc[0], loc[1])] # list of valid locations from above\n",
        "    # Final list checks for open locations, but allows current location\n",
        "    l_open_new_locs = [loc for loc in l_valid_new_locs if \n",
        "                       (a_impass[loc[1], loc[0]] == False) or ((loc[0], loc[1]) == (x, y))]\n",
        "\n",
        "    # We need the combined array of both bot teams\n",
        "    a_bot_idxs = np.argwhere(a_bots_blue + a_bots_red)\n",
        "\n",
        "    # List to append distance calculations into\n",
        "    l_max_dist = []\n",
        "\n",
        "    # Run through each new possible location and find distincance\n",
        "    for new_loc in l_open_new_locs:\n",
        "\n",
        "        # Find distances to all bots\n",
        "        l_bot_dist = [euclidean_dist(new_loc[0], new_loc[1], bot_loc[1], bot_loc[0]) for bot_loc in a_bot_idxs]\n",
        "        l_bot_dist = [dist for dist in l_bot_dist if dist <= vision_size] # only allow targets to \"see\" so far\n",
        "\n",
        "        # And the minimum distance to add to our list\n",
        "        if len(l_bot_dist) == 0: # In the case that there are no bots within vision limit\n",
        "            min_dist = np.inf\n",
        "        else:\n",
        "            min_dist = np.min(l_bot_dist)\n",
        "        l_max_dist.append(min_dist)\n",
        "\n",
        "    # And finally find the best target location (which pushes away close bots)\n",
        "    if len(set(l_max_dist)) == 1: # all choices are equivalent\n",
        "        best_loc = (x, y) # just stay put\n",
        "    else: # else pick the one maximizing the distance\n",
        "        best_loc = l_open_new_locs[np.argmax(l_max_dist)]\n",
        "\n",
        "    return best_loc\n",
        "\n",
        "# Actual function to move the targets on each timestep\n",
        "def move_target(t):\n",
        "\n",
        "    # First check if the stun countdown is in effect\n",
        "    if t.stun_countdown > 0:\n",
        "        \n",
        "        # If so, increment it down one\n",
        "        t.stun_countdown -= 1\n",
        "    \n",
        "    # Else move the target\n",
        "    else:\n",
        "\n",
        "        # Get the current location\n",
        "        x_curr, y_curr = t.x_loc, t.y_loc\n",
        "\n",
        "        # Get the new location (may be same as previous)\n",
        "        x_new, y_new = get_best_target_loc(x_curr, y_curr)\n",
        "\n",
        "        # Set it on the object\n",
        "        t.x_loc = x_new\n",
        "        t.y_loc = y_new\n",
        "\n",
        "        # Update all positional arrays\n",
        "        a_impass[y_curr, x_curr] = False\n",
        "        a_impass[y_new, x_new] = True\n",
        "        a_targets[y_curr, x_curr] = False\n",
        "        a_targets[y_new, x_new] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8rDmvNCc7A3"
      },
      "outputs": [],
      "source": [
        "# Function to move a bot given x,y adjustments\n",
        "def move_bot(b, x_adj, y_adj):\n",
        "\n",
        "    global red_team_raw_score_ts, blue_team_raw_score_ts\n",
        "    \n",
        "    # Get the current status of the bot\n",
        "    b_team = b.team\n",
        "    x_curr = b.x_loc\n",
        "    y_curr = b.y_loc\n",
        "\n",
        "    # Get theoretical new position\n",
        "    x_new = x_curr + x_adj\n",
        "    y_new = y_curr + y_adj\n",
        "    \n",
        "    # Run some true/false checks to make sure the move is valid\n",
        "    loc_is_valid = is_valid_loc(x_new, y_new)\n",
        "    if loc_is_valid:\n",
        "        xy_not_taken = (a_impass[y_new, x_new] == False)\n",
        "    else:\n",
        "        xy_not_taken = False\n",
        "    valid_move = loc_is_valid and xy_not_taken\n",
        "    \n",
        "    # If the move is valid, make it and run updates\n",
        "    if valid_move:\n",
        "\n",
        "        # Update the impass grid and the bots new position\n",
        "        a_impass[y_curr, x_curr] = False\n",
        "        a_impass[y_new, x_new] = True\n",
        "        b.x_loc = x_new\n",
        "        b.y_loc = y_new\n",
        "\n",
        "        # Update the team-specific bot array\n",
        "        if b_team == 'red':\n",
        "            a_bots_red[y_curr, x_curr] = False\n",
        "            a_bots_red[y_new, x_new] = True\n",
        "        else:\n",
        "            a_bots_blue[y_curr, x_curr] = False\n",
        "            a_bots_blue[y_new, x_new] = True\n",
        "\n",
        "        # Find the current and new distances to the closest intended object\n",
        "        if b_team == 'red':\n",
        "            # red follows targets\n",
        "            object_dist_curr = shortest_object_dist(x_curr, y_curr, a_targets)\n",
        "            object_dist_new = shortest_object_dist(x_new, y_new, a_targets)\n",
        "        else:\n",
        "            # blue follows red\n",
        "            object_dist_curr = shortest_object_dist(x_curr, y_curr, a_bots_red)\n",
        "            object_dist_new = shortest_object_dist(x_new, y_new, a_bots_red)\n",
        "\n",
        "        # If the new distance is shorter, set to add a reward\n",
        "        if object_dist_new < object_dist_curr:\n",
        "            move_reward = reward_closer\n",
        "        elif object_dist_new > object_dist_curr:\n",
        "            move_reward = reward_farther\n",
        "        else:\n",
        "            move_reward = 0\n",
        "\n",
        "    # If the move is invalid, penalize by the invalid reward\n",
        "    else:\n",
        "        move_reward = reward_invalid\n",
        "\n",
        "    # Update the bot's running timestep reward as well as that of the team scores\n",
        "    if b_team == 'red' and reward_red:\n",
        "        b.current_ts_reward += move_reward\n",
        "        red_team_raw_score_ts += move_reward\n",
        "    else:\n",
        "        # blue team gradually decays the move reward over time\n",
        "        b.current_ts_reward += move_reward * reward_blue_move_mult\n",
        "        blue_team_raw_score_ts += move_reward * reward_blue_move_mult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZqWQScCc7A3"
      },
      "outputs": [],
      "source": [
        "# Function for a bot to attemp the target at a given x,y coord\n",
        "def attempt_target(b, x_adj, y_adj):\n",
        "\n",
        "    # Only red team will get points for acquiring targets\n",
        "    global red_team_raw_score_ts, red_team_acquired_targets_ts\n",
        "    \n",
        "    # Get the current status of the bot\n",
        "    b_team = b.team\n",
        "    x_curr = b.x_loc\n",
        "    y_curr = b.y_loc\n",
        "\n",
        "    # Get theoretical new position\n",
        "    x_attempt = x_curr + x_adj\n",
        "    y_attempt = y_curr + y_adj\n",
        "\n",
        "    # Possibility to give a base reward for simply attempting the target\n",
        "    target_reward = reward_target_attempt\n",
        "\n",
        "    # Simple tracker for if the target works\n",
        "    target_secured = False\n",
        "    \n",
        "    # Run some true/false checks to see if attempt works\n",
        "    loc_is_valid = is_valid_loc(x_attempt, y_attempt)\n",
        "    if loc_is_valid:\n",
        "        target_available = (a_targets[y_attempt, x_attempt] == True)\n",
        "    else:\n",
        "        target_available = False\n",
        "    valid_target = loc_is_valid and target_available \n",
        "    \n",
        "    # If the target is valid, make it and run updates\n",
        "    if valid_target:\n",
        "        \n",
        "        # Find and remove the target at that location\n",
        "        for i, t in enumerate(l_targets):\n",
        "            if (t.x_loc == x_attempt) and (t.y_loc == y_attempt):\n",
        "                del l_targets[i]\n",
        "                break\n",
        "\n",
        "        # Update the impass grid and the targets grid\n",
        "        a_impass[y_attempt, x_attempt] = False\n",
        "        a_targets[y_attempt, x_attempt] = False\n",
        "\n",
        "        # Set the attempt target reward\n",
        "        target_reward += reward_target\n",
        "\n",
        "        # Note the target was secured\n",
        "        target_secured = True\n",
        "\n",
        "    # Else penalize for the invalid target\n",
        "    else:\n",
        "        if reward_red:\n",
        "            target_reward += reward_invalid\n",
        "\n",
        "    # Update the bot's running timestep reward as well as that of the team scores (red team only)\n",
        "    if b_team == 'red':\n",
        "        b.current_ts_reward += target_reward\n",
        "        red_team_raw_score_ts += target_reward\n",
        "        if target_secured:\n",
        "            red_team_acquired_targets_ts += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7twWB8FlZ_QZ"
      },
      "outputs": [],
      "source": [
        "# Function for the bot to create a stun set\n",
        "def create_stun(b, x_adj, y_adj):\n",
        "\n",
        "    # Only blue team gets points directly for stunning targets\n",
        "    global blue_team_raw_score_ts, blue_team_stunned_targets_ts\n",
        "\n",
        "    # Get the current status of the bot\n",
        "    b_team = b.team\n",
        "    x_curr = b.x_loc\n",
        "    y_curr = b.y_loc\n",
        "\n",
        "    # Determine the appropriate stun length\n",
        "    if b_team == 'red':\n",
        "        stun_dist = stun_dist_red\n",
        "    else:\n",
        "        stun_dist = stun_dist_blue\n",
        "\n",
        "    # Allow for reward for simply attempting a stun\n",
        "    stun_reward = reward_stun_attempt\n",
        "\n",
        "    # Create a list of possible stun locations\n",
        "    l_possible_stun_locs = [(x_curr + (x_adj * i), y_curr + (y_adj * i)) for i in range(1, stun_dist + 1)]\n",
        "\n",
        "    # Create actual list of stun locs (break on first nonviable space)\n",
        "    # Store the last loc index so we can find what exists at the end of the stun\n",
        "    l_stun_locs = []\n",
        "    last_loc_index = 0 # technically this ends up as the last valid idx + 1\n",
        "    for loc in l_possible_stun_locs:\n",
        "        if not is_valid_loc(loc[0], loc[1]):\n",
        "            break\n",
        "        else:\n",
        "            if a_impass[loc[1], loc[0]] == True:\n",
        "                break\n",
        "            else:\n",
        "                last_loc_index += 1\n",
        "                l_stun_locs.append(loc)\n",
        "\n",
        "    # Calculate the position one block farther than the stuns end\n",
        "    stun_end_x_loc = x_curr + (x_adj * (last_loc_index + 1))\n",
        "    stun_end_y_loc = y_curr + (y_adj * (last_loc_index + 1))\n",
        "\n",
        "    # Now check if that next location is both valid and if it has a target (to decide if a taret was \"hit\")\n",
        "    target_hit = False\n",
        "    if is_valid_loc(stun_end_x_loc, stun_end_y_loc):\n",
        "        if a_targets[stun_end_y_loc, stun_end_x_loc] == True:\n",
        "            target_hit = True\n",
        "            target_loc = (stun_end_x_loc, stun_end_y_loc)\n",
        "\n",
        "    # Handling if we actually hit a target\n",
        "    if target_hit:\n",
        "\n",
        "        # Handle the rewards\n",
        "        # Don't hit the exact same position twice in a row\n",
        "        if b.previous_stun_loc == target_loc:\n",
        "            stun_reward += reward_stun_same\n",
        "\n",
        "        # Otherwise give the base reward and check for the proximity reward\n",
        "        else:\n",
        "            # base reward\n",
        "            stun_reward += reward_stun\n",
        "            # create an array centered on the target location to see if a red bot is nearby (the vision arrays are convenient here)\n",
        "            a_stun_proximity = vision_a(a_bots_red, target_loc[0], target_loc[1], vision_limit=stun_proximity_cap)\n",
        "            # if a red bot (will show as 1) is in that array (capped by the proximity limit), add the extra reward\n",
        "            if 1 in a_stun_proximity:\n",
        "                stun_reward += reward_stun_proximity\n",
        "\n",
        "\n",
        "        # Find and set the stun counter for the target at that location\n",
        "        for t in l_targets:\n",
        "            if (t.x_loc == target_loc[0]) and (t.y_loc == target_loc[1]):\n",
        "                t.stun_countdown = stun_duration\n",
        "                break\n",
        "\n",
        "        # Update the tracker of last location the bot stunned\n",
        "        b.previous_stun_loc = target_loc\n",
        "\n",
        "    # If a valid stun array was created but no target was hit, we can add the miss reward\n",
        "    elif (not target_hit) and (len(l_stun_locs) != 0):\n",
        "\n",
        "        stun_reward += reward_stun_miss\n",
        "\n",
        "    # If the stun move didn't make sense (the list is empty and no target hit), penalize by the invalid reward\n",
        "    else:\n",
        "        stun_reward += reward_invalid\n",
        "\n",
        "    # Now actually create the stuns in our array\n",
        "    # It's technically possible a target was stunned when the array length is 0 (if right next to it), but we won't bother to show\n",
        "    if len(l_stun_locs) != 0:\n",
        "        for loc in l_stun_locs:\n",
        "            a_stuns[loc[1], loc[0]] = True\n",
        "\n",
        "   # Update the bot's running timestep reward as well as that of the team scores (blue team only)\n",
        "    if b_team == 'blue':\n",
        "        b.current_ts_reward += stun_reward\n",
        "        blue_team_raw_score_ts += stun_reward\n",
        "        if target_hit:\n",
        "            blue_team_stunned_targets_ts += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXex7IBsN--7"
      },
      "outputs": [],
      "source": [
        "# Function for the bot doing nothing, needed for debugging and score updates\n",
        "def do_nothing(b):\n",
        "\n",
        "    global red_team_raw_score_ts, blue_team_raw_score_ts\n",
        "\n",
        "    # Update the bot's running timestep reward as well as that of the team scores\n",
        "    if b.team == 'red' and reward_red:\n",
        "        b.current_ts_reward += reward_inaction\n",
        "        red_team_raw_score_ts += reward_inaction\n",
        "    else:\n",
        "        b.current_ts_reward += reward_inaction\n",
        "        blue_team_raw_score_ts += reward_inaction        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRcoTpMQ5bZn"
      },
      "outputs": [],
      "source": [
        "# Function to remove stuns from their array\n",
        "def clean_stuns():\n",
        "\n",
        "    # This is really just a visual effect so we can essentially recreate the array\n",
        "    global a_stuns\n",
        "    a_stuns = init_a(grid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "civ9z23FCJdX"
      },
      "outputs": [],
      "source": [
        "# Function to restore number of targets back to the original amount\n",
        "def replenish_targets():\n",
        "    n_remaining_targets = len(l_targets)\n",
        "    if n_remaining_targets < n_targets:\n",
        "        n_replenish = n_targets - n_remaining_targets\n",
        "        init_positions(target, l_targets, a_targets, n_replenish)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwECgmPCc7A3"
      },
      "outputs": [],
      "source": [
        "# General handler function to update the bot at each timestep\n",
        "def update_bot(b):\n",
        "\n",
        "    global red_team_raw_score_ts, blue_team_raw_score_ts\n",
        "\n",
        "    # Reset the current timestep reward\n",
        "    b.current_ts_reward = 0\n",
        "\n",
        "    # Grab current location and inputs\n",
        "    x_curr, y_curr = b.x_loc, b.y_loc\n",
        "    vision_input = prep_vision(x_curr, y_curr).to(device)\n",
        "\n",
        "    # Determine the bots action decision and map to action type/directions\n",
        "    if b.team == 'red':\n",
        "        action_idx, action_logp, critic_value, b.lstm_1_h, b.lstm_1_c = team_brain_red.decide_action(vision_input, b.lstm_1_h, b.lstm_1_c) \n",
        "        b.lstm_1_h = b.lstm_1_h.detach()\n",
        "        b.lstm_1_c = b.lstm_1_c.detach()\n",
        "    else:\n",
        "        action_idx, action_logp, critic_value, b.lstm_1_h, b.lstm_1_c = team_brain_blue.decide_action(vision_input, b.lstm_1_h, b.lstm_1_c) \n",
        "        b.lstm_1_h = b.lstm_1_h.detach()\n",
        "        b.lstm_1_c = b.lstm_1_c.detach()\n",
        "\n",
        "    x_adj, y_adj = map_action_direction(action_idx)\n",
        "    action_type = map_action_type(action_idx)\n",
        "\n",
        "    # Keep track of any items we need in the bots internal lists\n",
        "    b.l_episode_log_probs.append(action_logp)\n",
        "    b.l_type_choices.append(action_type)\n",
        "    b.l_episode_critic_values.append(critic_value)\n",
        "\n",
        "    # Handle any constant reward modifications\n",
        "    if b.team == 'red':\n",
        "        red_constant_reward = reward_constant_penalty\n",
        "        if reward_red:\n",
        "            entropy_reward = entropy_loss(b.l_type_choices)\n",
        "            if action_type == 'target':\n",
        "                    b.l_dir_choices.append((x_adj, y_adj))\n",
        "                    entropy_reward += entropy_loss(b.l_dir_choices, mult=entropy_mult_dir)\n",
        "            red_constant_reward += entropy_reward\n",
        "        b.current_ts_reward += red_constant_reward\n",
        "        red_team_raw_score_ts += red_constant_reward\n",
        "    else:\n",
        "        entropy_reward = entropy_loss(b.l_type_choices)\n",
        "        if action_type == 'stun':\n",
        "                b.l_dir_choices.append((x_adj, y_adj))\n",
        "                entropy_reward += entropy_loss(b.l_dir_choices, mult=entropy_mult_dir)\n",
        "        b.current_ts_reward += entropy_reward + reward_constant_penalty\n",
        "        blue_team_raw_score_ts += entropy_reward + reward_constant_penalty\n",
        "    \n",
        "    # Perform the decided bot action\n",
        "    if action_type == 'none':\n",
        "        do_nothing(b)\n",
        "    elif action_type == 'move':\n",
        "        move_bot(b, x_adj, y_adj)\n",
        "    elif action_type == 'target':\n",
        "        attempt_target(b, x_adj, y_adj)\n",
        "    elif action_type == 'stun':\n",
        "        create_stun(b, x_adj, y_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P_5XYNLryeO"
      },
      "outputs": [],
      "source": [
        "# Function to calculate loss and update bot network\n",
        "def reinforce_botbrain(bot_brain, bot_list, bot_optim):\n",
        "\n",
        "    l_total_loss = []\n",
        "\n",
        "    for b in bot_list:\n",
        "\n",
        "        # Setup lists and vars to work off\n",
        "        discounted_reward = 0\n",
        "        l_returns = []\n",
        "        l_policy_loss = []\n",
        "        l_critic_loss = []\n",
        "\n",
        "        # Work through the bot's episode rewards backwards\n",
        "        # The net effect of this will be such that we built rewards for only actions and their following rewards\n",
        "        # (i.e. action for step n only gets rewards for steps > n, never steps < n)\n",
        "        # Additionally we'll build in our reward discounting (where future steps contribute less to overall reward)\n",
        "        for reward in b.l_episode_rewards[::-1]:\n",
        "            discounted_reward = reward + gamma * discounted_reward\n",
        "            l_returns.insert(0, discounted_reward) # but insert back at the beginning to get correct order\n",
        "        \n",
        "        # Now turn the rewards into a tensor for working with gradient\n",
        "        t_returns = torch.tensor(l_returns)\n",
        "        # But standardize the rewards to stabilize training\n",
        "        t_returns = (t_returns - t_returns.mean()) / (t_returns.std() + 1e-6)\n",
        "\n",
        "        # Now build up our actual policy loss by multiplying it by our logprobs\n",
        "        for logp, critic_val, discounted_reward in zip(b.l_episode_log_probs, b.l_episode_critic_values, t_returns):\n",
        "            \n",
        "            advantage = discounted_reward - critic_val.item()\n",
        "\n",
        "            l_policy_loss.append(-logp * advantage)\n",
        "\n",
        "            critic_loss_timestep = F.smooth_l1_loss(critic_val.cpu(), torch.tensor([discounted_reward]).view(1,1)) # view needed as critic is size(1,1) instead of (1)\n",
        "            l_critic_loss.append(critic_loss_timestep.view(1)) # view here makes output match behavior of policy loss, size(1)\n",
        "\n",
        "        # Technically our l_policy_loss is a list of tensors, so smoosh those together\n",
        "        # Then sum to get the total loss\n",
        "        policy_loss = torch.cat(l_policy_loss).sum()\n",
        "        critic_loss = torch.cat(l_critic_loss).sum()\n",
        "        l_total_loss.append(policy_loss + critic_loss)\n",
        "\n",
        "        # And cleanup our epsiode tracking lists now since we don't need them\n",
        "        del b.l_episode_rewards[:]\n",
        "        del b.l_episode_log_probs[:]\n",
        "        del b.l_episode_critic_values[:]\n",
        "\n",
        "    # Zero our gradient to get ready for backprop\n",
        "    bot_optim.zero_grad()\n",
        "\n",
        "    # Create our collective policy loss\n",
        "    loss_collective = torch.stack(l_total_loss)\n",
        "    loss_collective = loss_collective.mean().to(device)\n",
        "\n",
        "    # Now run our optimizer\n",
        "    loss_collective.backward()\n",
        "    bot_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZCpvrAKzz2T"
      },
      "outputs": [],
      "source": [
        "# Function to reset any needed items in the environment\n",
        "def reset_environment():\n",
        "\n",
        "    # Clear all stuns\n",
        "    clean_stuns()\n",
        "\n",
        "    # Reset positions\n",
        "    reset_positions(l_bots_red, a_bots_red)\n",
        "    reset_positions(l_bots_blue, a_bots_blue)\n",
        "    reset_positions(l_targets, a_targets)\n",
        "\n",
        "    # Reset individual bot attributes\n",
        "    for b in l_bots_red:\n",
        "        b.reset_memory()\n",
        "        del b.l_type_choices[:]\n",
        "        del b.l_dir_choices[:]\n",
        "        b.previous_stun_loc = None\n",
        "    for b in l_bots_blue:\n",
        "        b.reset_memory()\n",
        "        del b.l_type_choices[:]\n",
        "        del b.l_dir_choices[:]\n",
        "        b.previous_stun_loc = None\n",
        "\n",
        "    # Reset any target attributes\n",
        "    for t in l_targets:\n",
        "        t.stun_countdown = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i3FxVPzc7A4",
        "outputId": "61c82722-77be-48e3-88e2-25769b40dc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished episode 500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/15.14/0.14.  Blue team obj/raw/obj-mov/raw-mov score:  17/191.415/16.22/163.912.\n",
            "Finished episode 1000.  Red team obj/raw/obj-mov/raw-mov score:  11/-4.0/14.86/-0.14.  Blue team obj/raw/obj-mov/raw-mov score:  16/165.277/13.78/145.107.\n",
            "Finished episode 1500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.2/1.2.  Blue team obj/raw/obj-mov/raw-mov score:  18/206.115/17.94/172.206.\n",
            "Finished episode 2000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/14.92/-0.08.  Blue team obj/raw/obj-mov/raw-mov score:  13/126.738/17.58/172.477.\n",
            "Finished episode 2500.  Red team obj/raw/obj-mov/raw-mov score:  11/-4.0/14.94/-0.06.  Blue team obj/raw/obj-mov/raw-mov score:  13/133.714/15.7/169.954.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 3000.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/15.14/0.14.  Blue team obj/raw/obj-mov/raw-mov score:  19/208.136/18.14/150.277.\n",
            "Finished episode 3500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.66/0.66.  Blue team obj/raw/obj-mov/raw-mov score:  28/116.373/21.42/146.521.\n",
            "Finished episode 4000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/14.72/-0.28.  Blue team obj/raw/obj-mov/raw-mov score:  27/165.801/17.6/165.741.\n",
            "Finished episode 4500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/14.8/-0.2.  Blue team obj/raw/obj-mov/raw-mov score:  14/159.365/18.5/161.706.\n",
            "Finished episode 5000.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/15.22/0.22.  Blue team obj/raw/obj-mov/raw-mov score:  15/141.216/21.18/166.069.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 5500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.26/0.26.  Blue team obj/raw/obj-mov/raw-mov score:  24/187.345/19.36/175.092.\n",
            "Finished episode 6000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.58/0.58.  Blue team obj/raw/obj-mov/raw-mov score:  20/143.677/18.36/161.79.\n",
            "Finished episode 6500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.12/0.12.  Blue team obj/raw/obj-mov/raw-mov score:  9/60.309/18.14/167.114.\n",
            "Finished episode 7000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.0/-0.0.  Blue team obj/raw/obj-mov/raw-mov score:  16/102.067/17.28/172.04.\n",
            "Finished episode 7500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.78/0.78.  Blue team obj/raw/obj-mov/raw-mov score:  15/142.074/18.62/186.22.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 8000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.24/0.24.  Blue team obj/raw/obj-mov/raw-mov score:  15/192.157/18.14/188.497.\n",
            "Finished episode 8500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/14.92/-0.08.  Blue team obj/raw/obj-mov/raw-mov score:  16/244.858/16.22/171.887.\n",
            "Finished episode 9000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.9/0.9.  Blue team obj/raw/obj-mov/raw-mov score:  20/211.442/18.04/196.048.\n",
            "Finished episode 9500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.54/0.54.  Blue team obj/raw/obj-mov/raw-mov score:  22/269.826/16.4/168.646.\n",
            "Finished episode 10000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.44/0.44.  Blue team obj/raw/obj-mov/raw-mov score:  25/273.938/17.52/175.948.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 10500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.5/0.5.  Blue team obj/raw/obj-mov/raw-mov score:  20/172.383/16.38/181.0.\n",
            "Finished episode 11000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.58/0.58.  Blue team obj/raw/obj-mov/raw-mov score:  19/168.817/20.12/176.179.\n",
            "Finished episode 11500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/14.9/-0.1.  Blue team obj/raw/obj-mov/raw-mov score:  13/161.231/14.76/159.585.\n",
            "Finished episode 12000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.7/0.7.  Blue team obj/raw/obj-mov/raw-mov score:  21/176.199/20.24/184.589.\n",
            "Finished episode 12500.  Red team obj/raw/obj-mov/raw-mov score:  12/-3.0/15.32/0.32.  Blue team obj/raw/obj-mov/raw-mov score:  11/133.476/18.54/146.456.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 13000.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/15.1/0.1.  Blue team obj/raw/obj-mov/raw-mov score:  20/147.719/17.58/168.288.\n",
            "Finished episode 13500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.78/0.78.  Blue team obj/raw/obj-mov/raw-mov score:  24/257.59/19.52/181.118.\n",
            "Finished episode 14000.  Red team obj/raw/obj-mov/raw-mov score:  11/-4.0/15.62/0.62.  Blue team obj/raw/obj-mov/raw-mov score:  24/95.02/18.36/176.234.\n",
            "Finished episode 14500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/16.24/1.24.  Blue team obj/raw/obj-mov/raw-mov score:  18/172.621/19.1/187.021.\n",
            "Finished episode 15000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.68/0.68.  Blue team obj/raw/obj-mov/raw-mov score:  21/243.021/17.72/167.582.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 15500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/11.12/-3.88.  Blue team obj/raw/obj-mov/raw-mov score:  17/214.635/13.26/111.883.\n",
            "Finished episode 16000.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/15.76/0.76.  Blue team obj/raw/obj-mov/raw-mov score:  18/219.731/18.62/179.671.\n",
            "Finished episode 16500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/15.96/0.96.  Blue team obj/raw/obj-mov/raw-mov score:  23/254.307/18.24/175.724.\n",
            "Finished episode 17000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.74/0.74.  Blue team obj/raw/obj-mov/raw-mov score:  15/190.836/17.16/175.85.\n",
            "Finished episode 17500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.26/1.26.  Blue team obj/raw/obj-mov/raw-mov score:  14/104.305/19.1/192.483.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 18000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.06/1.06.  Blue team obj/raw/obj-mov/raw-mov score:  12/117.034/18.02/192.249.\n",
            "Finished episode 18500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/15.88/0.88.  Blue team obj/raw/obj-mov/raw-mov score:  22/233.898/18.7/195.335.\n",
            "Finished episode 19000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/15.94/0.94.  Blue team obj/raw/obj-mov/raw-mov score:  19/236.827/18.7/200.566.\n",
            "Finished episode 19500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/13.08/-1.92.  Blue team obj/raw/obj-mov/raw-mov score:  17/184.026/15.36/138.867.\n",
            "Finished episode 20000.  Red team obj/raw/obj-mov/raw-mov score:  10/-5.0/15.58/0.58.  Blue team obj/raw/obj-mov/raw-mov score:  13/157.424/16.88/187.702.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 20500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.72/0.72.  Blue team obj/raw/obj-mov/raw-mov score:  15/128.509/20.2/175.048.\n",
            "Finished episode 21000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.06/1.06.  Blue team obj/raw/obj-mov/raw-mov score:  20/229.051/19.58/189.788.\n",
            "Finished episode 21500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/15.74/0.74.  Blue team obj/raw/obj-mov/raw-mov score:  17/149.452/18.8/187.039.\n",
            "Finished episode 22000.  Red team obj/raw/obj-mov/raw-mov score:  12/-3.0/14.72/-0.28.  Blue team obj/raw/obj-mov/raw-mov score:  18/109.183/16.9/165.189.\n",
            "Finished episode 22500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/14.98/-0.02.  Blue team obj/raw/obj-mov/raw-mov score:  22/240.406/16.84/182.398.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 23000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/15.82/0.82.  Blue team obj/raw/obj-mov/raw-mov score:  16/224.098/19.76/186.135.\n",
            "Finished episode 23500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.6/0.6.  Blue team obj/raw/obj-mov/raw-mov score:  18/212.292/17.26/184.933.\n",
            "Finished episode 24000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.94/0.94.  Blue team obj/raw/obj-mov/raw-mov score:  21/252.462/18.06/191.311.\n",
            "Finished episode 24500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.58/0.58.  Blue team obj/raw/obj-mov/raw-mov score:  9/84.963/19.34/184.918.\n",
            "Finished episode 25000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.92/0.92.  Blue team obj/raw/obj-mov/raw-mov score:  21/186.316/17.98/185.42.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 25500.  Red team obj/raw/obj-mov/raw-mov score:  11/-4.0/15.7/0.7.  Blue team obj/raw/obj-mov/raw-mov score:  12/88.001/17.58/184.783.\n",
            "Finished episode 26000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/15.96/0.96.  Blue team obj/raw/obj-mov/raw-mov score:  14/193.693/19.42/183.668.\n",
            "Finished episode 26500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.18/0.18.  Blue team obj/raw/obj-mov/raw-mov score:  14/192.456/18.06/177.704.\n",
            "Finished episode 27000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.18/0.18.  Blue team obj/raw/obj-mov/raw-mov score:  16/170.881/19.26/175.434.\n",
            "Finished episode 27500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.84/0.84.  Blue team obj/raw/obj-mov/raw-mov score:  17/147.803/19.28/178.419.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 28000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.32/1.32.  Blue team obj/raw/obj-mov/raw-mov score:  13/135.082/21.48/205.204.\n",
            "Finished episode 28500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/15.46/0.46.  Blue team obj/raw/obj-mov/raw-mov score:  16/191.6/16.6/179.629.\n",
            "Finished episode 29000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.72/0.72.  Blue team obj/raw/obj-mov/raw-mov score:  13/129.172/16.44/172.15.\n",
            "Finished episode 29500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/15.66/0.66.  Blue team obj/raw/obj-mov/raw-mov score:  21/179.06/18.42/182.65.\n",
            "Finished episode 30000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.82/0.82.  Blue team obj/raw/obj-mov/raw-mov score:  12/152.165/17.78/180.264.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 30500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/15.62/0.62.  Blue team obj/raw/obj-mov/raw-mov score:  18/205.739/18.6/188.393.\n",
            "Finished episode 31000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.22/1.22.  Blue team obj/raw/obj-mov/raw-mov score:  13/124.508/20.48/182.006.\n",
            "Finished episode 31500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/14.64/-0.36.  Blue team obj/raw/obj-mov/raw-mov score:  11/115.912/14.66/163.279.\n",
            "Finished episode 32000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.14/1.14.  Blue team obj/raw/obj-mov/raw-mov score:  17/221.588/17.32/184.438.\n",
            "Finished episode 32500.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/16.44/1.44.  Blue team obj/raw/obj-mov/raw-mov score:  7/83.796/17.48/192.151.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 33000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.04/1.04.  Blue team obj/raw/obj-mov/raw-mov score:  17/75.59/17.54/176.965.\n",
            "Finished episode 33500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/17.32/2.32.  Blue team obj/raw/obj-mov/raw-mov score:  23/269.335/21.44/204.403.\n",
            "Finished episode 34000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/13.16/-1.84.  Blue team obj/raw/obj-mov/raw-mov score:  14/155.405/14.44/160.518.\n",
            "Finished episode 34500.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/15.9/0.9.  Blue team obj/raw/obj-mov/raw-mov score:  21/164.048/19.3/179.892.\n",
            "Finished episode 35000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.26/1.26.  Blue team obj/raw/obj-mov/raw-mov score:  24/219.162/18.76/199.772.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 35500.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.54/1.54.  Blue team obj/raw/obj-mov/raw-mov score:  24/191.15/18.44/198.183.\n",
            "Finished episode 36000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/16.24/1.24.  Blue team obj/raw/obj-mov/raw-mov score:  21/192.984/19.98/196.547.\n",
            "Finished episode 36500.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/16.68/1.68.  Blue team obj/raw/obj-mov/raw-mov score:  20/176.661/18.0/194.866.\n",
            "Finished episode 37000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/16.26/1.26.  Blue team obj/raw/obj-mov/raw-mov score:  23/247.199/18.38/191.471.\n",
            "Finished episode 37500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/14.98/-0.02.  Blue team obj/raw/obj-mov/raw-mov score:  22/192.733/16.74/177.889.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 38000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.3/0.3.  Blue team obj/raw/obj-mov/raw-mov score:  19/208.325/16.9/188.166.\n",
            "Finished episode 38500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.1/1.1.  Blue team obj/raw/obj-mov/raw-mov score:  14/102.386/17.44/177.123.\n",
            "Finished episode 39000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.82/0.82.  Blue team obj/raw/obj-mov/raw-mov score:  22/246.858/18.36/181.303.\n",
            "Finished episode 39500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.14/1.14.  Blue team obj/raw/obj-mov/raw-mov score:  17/184.448/18.76/198.773.\n",
            "Finished episode 40000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.66/0.66.  Blue team obj/raw/obj-mov/raw-mov score:  22/223.152/17.54/187.335.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 40500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.26/1.26.  Blue team obj/raw/obj-mov/raw-mov score:  16/157.321/19.08/194.529.\n",
            "Finished episode 41000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.8/0.8.  Blue team obj/raw/obj-mov/raw-mov score:  14/131.848/16.66/191.037.\n",
            "Finished episode 41500.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.8/1.8.  Blue team obj/raw/obj-mov/raw-mov score:  21/257.578/19.1/193.334.\n",
            "Finished episode 42000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.12/1.12.  Blue team obj/raw/obj-mov/raw-mov score:  22/245.069/18.98/188.303.\n",
            "Finished episode 42500.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.16/1.16.  Blue team obj/raw/obj-mov/raw-mov score:  12/169.841/18.26/193.909.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 43000.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.66/1.66.  Blue team obj/raw/obj-mov/raw-mov score:  28/277.264/18.44/205.278.\n",
            "Finished episode 43500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/15.28/0.28.  Blue team obj/raw/obj-mov/raw-mov score:  23/214.766/18.0/182.691.\n",
            "Finished episode 44000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.76/1.76.  Blue team obj/raw/obj-mov/raw-mov score:  14/157.991/19.66/214.332.\n",
            "Finished episode 44500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.2/1.2.  Blue team obj/raw/obj-mov/raw-mov score:  22/224.683/17.48/206.595.\n",
            "Finished episode 45000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.32/1.32.  Blue team obj/raw/obj-mov/raw-mov score:  17/182.176/20.1/202.874.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 45500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.18/1.18.  Blue team obj/raw/obj-mov/raw-mov score:  17/181.17/18.12/193.409.\n",
            "Finished episode 46000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.44/1.44.  Blue team obj/raw/obj-mov/raw-mov score:  15/175.009/17.6/202.99.\n",
            "Finished episode 46500.  Red team obj/raw/obj-mov/raw-mov score:  8/-7.0/8.92/-6.08.  Blue team obj/raw/obj-mov/raw-mov score:  32/100.146/17.34/81.73.\n",
            "Finished episode 47000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.62/0.62.  Blue team obj/raw/obj-mov/raw-mov score:  19/235.459/19.18/193.037.\n",
            "Finished episode 47500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.3/1.3.  Blue team obj/raw/obj-mov/raw-mov score:  26/170.851/18.66/195.469.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 48000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.06/1.06.  Blue team obj/raw/obj-mov/raw-mov score:  12/127.643/17.46/189.052.\n",
            "Finished episode 48500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/16.04/1.04.  Blue team obj/raw/obj-mov/raw-mov score:  18/173.414/18.24/179.471.\n",
            "Finished episode 49000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.46/1.46.  Blue team obj/raw/obj-mov/raw-mov score:  14/189.837/19.78/189.285.\n",
            "Finished episode 49500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/15.64/0.64.  Blue team obj/raw/obj-mov/raw-mov score:  32/247.467/20.64/209.167.\n",
            "Finished episode 50000.  Red team obj/raw/obj-mov/raw-mov score:  10/-5.0/16.52/1.52.  Blue team obj/raw/obj-mov/raw-mov score:  13/104.822/19.74/201.665.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 50500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/15.74/0.74.  Blue team obj/raw/obj-mov/raw-mov score:  22/234.567/19.3/208.155.\n",
            "Finished episode 51000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.44/1.44.  Blue team obj/raw/obj-mov/raw-mov score:  16/163.139/18.32/193.23.\n",
            "Finished episode 51500.  Red team obj/raw/obj-mov/raw-mov score:  12/-3.0/16.12/1.12.  Blue team obj/raw/obj-mov/raw-mov score:  15/146.077/18.6/204.612.\n",
            "Finished episode 52000.  Red team obj/raw/obj-mov/raw-mov score:  8/-7.0/15.52/0.52.  Blue team obj/raw/obj-mov/raw-mov score:  6/48.788/19.96/188.782.\n",
            "Finished episode 52500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/15.74/0.74.  Blue team obj/raw/obj-mov/raw-mov score:  28/205.235/19.42/191.589.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 53000.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.3/1.3.  Blue team obj/raw/obj-mov/raw-mov score:  18/194.359/20.94/205.228.\n",
            "Finished episode 53500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.46/1.46.  Blue team obj/raw/obj-mov/raw-mov score:  16/211.851/19.76/210.868.\n",
            "Finished episode 54000.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/14.84/-0.16.  Blue team obj/raw/obj-mov/raw-mov score:  25/151.432/20.4/195.228.\n",
            "Finished episode 54500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/17.06/2.06.  Blue team obj/raw/obj-mov/raw-mov score:  20/225.334/21.8/205.819.\n",
            "Finished episode 55000.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/16.82/1.82.  Blue team obj/raw/obj-mov/raw-mov score:  15/119.792/21.22/211.734.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 55500.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/17.46/2.46.  Blue team obj/raw/obj-mov/raw-mov score:  25/139.104/21.24/219.285.\n",
            "Finished episode 56000.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/17.0/2.0.  Blue team obj/raw/obj-mov/raw-mov score:  19/162.386/21.9/224.942.\n",
            "Finished episode 56500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.7/1.7.  Blue team obj/raw/obj-mov/raw-mov score:  17/152.901/20.7/196.416.\n",
            "Finished episode 57000.  Red team obj/raw/obj-mov/raw-mov score:  10/-5.0/15.32/0.32.  Blue team obj/raw/obj-mov/raw-mov score:  11/131.305/17.66/201.346.\n",
            "Finished episode 57500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.5/1.5.  Blue team obj/raw/obj-mov/raw-mov score:  18/201.22/19.72/209.952.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 58000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/15.96/0.96.  Blue team obj/raw/obj-mov/raw-mov score:  27/293.967/19.0/183.019.\n",
            "Finished episode 58500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/17.4/2.4.  Blue team obj/raw/obj-mov/raw-mov score:  19/162.463/21.0/216.898.\n",
            "Finished episode 59000.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/16.2/1.2.  Blue team obj/raw/obj-mov/raw-mov score:  22/289.641/19.04/214.904.\n",
            "Finished episode 59500.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.38/1.38.  Blue team obj/raw/obj-mov/raw-mov score:  13/157.437/20.66/223.132.\n",
            "Finished episode 60000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.74/1.74.  Blue team obj/raw/obj-mov/raw-mov score:  21/101.557/21.74/200.771.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 60500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.16/1.16.  Blue team obj/raw/obj-mov/raw-mov score:  21/233.973/19.52/200.268.\n",
            "Finished episode 61000.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/15.98/0.98.  Blue team obj/raw/obj-mov/raw-mov score:  23/268.831/18.54/200.995.\n",
            "Finished episode 61500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.06/1.06.  Blue team obj/raw/obj-mov/raw-mov score:  14/206.861/18.76/199.756.\n",
            "Finished episode 62000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/17.08/2.08.  Blue team obj/raw/obj-mov/raw-mov score:  25/261.975/19.4/209.647.\n",
            "Finished episode 62500.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.1/1.1.  Blue team obj/raw/obj-mov/raw-mov score:  19/140.077/19.62/201.056.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 63000.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/17.3/2.3.  Blue team obj/raw/obj-mov/raw-mov score:  21/302.664/20.4/227.81.\n",
            "Finished episode 63500.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/16.3/1.3.  Blue team obj/raw/obj-mov/raw-mov score:  23/221.943/18.84/200.714.\n",
            "Finished episode 64000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/15.58/0.58.  Blue team obj/raw/obj-mov/raw-mov score:  19/210.343/19.64/198.458.\n",
            "Finished episode 64500.  Red team obj/raw/obj-mov/raw-mov score:  15/0.0/15.62/0.62.  Blue team obj/raw/obj-mov/raw-mov score:  12/149.536/17.68/185.46.\n",
            "Finished episode 65000.  Red team obj/raw/obj-mov/raw-mov score:  22/7.0/15.68/0.68.  Blue team obj/raw/obj-mov/raw-mov score:  18/262.131/17.42/176.346.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 65500.  Red team obj/raw/obj-mov/raw-mov score:  15/-0.0/16.92/1.92.  Blue team obj/raw/obj-mov/raw-mov score:  19/255.221/17.9/213.388.\n",
            "Finished episode 66000.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.42/1.42.  Blue team obj/raw/obj-mov/raw-mov score:  21/189.228/18.5/208.935.\n",
            "Finished episode 66500.  Red team obj/raw/obj-mov/raw-mov score:  20/5.0/16.6/1.6.  Blue team obj/raw/obj-mov/raw-mov score:  26/397.744/20.1/224.908.\n",
            "Finished episode 67000.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/16.68/1.68.  Blue team obj/raw/obj-mov/raw-mov score:  22/169.508/16.68/200.859.\n",
            "Finished episode 67500.  Red team obj/raw/obj-mov/raw-mov score:  14/-1.0/17.26/2.26.  Blue team obj/raw/obj-mov/raw-mov score:  20/186.116/20.44/210.59.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 68000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.88/1.88.  Blue team obj/raw/obj-mov/raw-mov score:  17/196.014/21.38/202.983.\n",
            "Finished episode 68500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.22/1.22.  Blue team obj/raw/obj-mov/raw-mov score:  20/190.503/19.94/194.346.\n",
            "Finished episode 69000.  Red team obj/raw/obj-mov/raw-mov score:  19/4.0/17.38/2.38.  Blue team obj/raw/obj-mov/raw-mov score:  35/196.705/21.12/221.982.\n",
            "Finished episode 69500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.58/1.58.  Blue team obj/raw/obj-mov/raw-mov score:  17/211.213/19.58/196.247.\n",
            "Finished episode 70000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.4/1.4.  Blue team obj/raw/obj-mov/raw-mov score:  16/185.003/20.76/192.036.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 70500.  Red team obj/raw/obj-mov/raw-mov score:  17/2.0/16.32/1.32.  Blue team obj/raw/obj-mov/raw-mov score:  24/239.504/19.16/204.435.\n",
            "Finished episode 71000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/17.24/2.24.  Blue team obj/raw/obj-mov/raw-mov score:  19/237.102/18.62/204.876.\n",
            "Finished episode 71500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.5/1.5.  Blue team obj/raw/obj-mov/raw-mov score:  24/278.688/18.6/204.993.\n",
            "Finished episode 72000.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.96/1.96.  Blue team obj/raw/obj-mov/raw-mov score:  19/225.005/19.9/210.665.\n",
            "Finished episode 72500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.38/1.38.  Blue team obj/raw/obj-mov/raw-mov score:  20/218.272/19.3/206.547.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Finished episode 73000.  Red team obj/raw/obj-mov/raw-mov score:  16/1.0/16.64/1.64.  Blue team obj/raw/obj-mov/raw-mov score:  14/162.496/19.72/207.112.\n",
            "Finished episode 73500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.08/1.08.  Blue team obj/raw/obj-mov/raw-mov score:  12/151.567/20.18/203.726.\n",
            "Finished episode 74000.  Red team obj/raw/obj-mov/raw-mov score:  13/-2.0/16.48/1.48.  Blue team obj/raw/obj-mov/raw-mov score:  18/153.983/20.14/210.163.\n",
            "Finished episode 74500.  Red team obj/raw/obj-mov/raw-mov score:  18/3.0/16.98/1.98.  Blue team obj/raw/obj-mov/raw-mov score:  26/192.07/21.3/211.05.\n",
            "Finished episode 75000.  Red team obj/raw/obj-mov/raw-mov score:  21/6.0/16.84/1.84.  Blue team obj/raw/obj-mov/raw-mov score:  26/325.969/20.74/209.734.\n",
            "Generating gif...\n",
            "Done building gif.\n",
            "Done with simultation.\n"
          ]
        }
      ],
      "source": [
        "# The main loop across n episodes\n",
        "for i_episode in range(0, n_episodes):\n",
        "\n",
        "    # Starting metrics for the episode\n",
        "    episode_length = l_episode_lengths[i_episode]\n",
        "    n_targets = l_n_targets[i_episode]\n",
        "    red_team_raw_score_episode = 0\n",
        "    blue_team_raw_score_episode = 0\n",
        "    red_team_acquired_targets_episode = 0\n",
        "    blue_team_stunned_targets_episode = 0\n",
        "\n",
        "    # Decay any rewards as desired\n",
        "    entropy_mult_main = entropy_mult_main * entropy_mult_decay\n",
        "    reward_blue_move_mult = reward_blue_move_mult * blue_mult_decay\n",
        "\n",
        "    # Actions to take at each timestep (n timesteps defined above)\n",
        "    for time_step in range(episode_length):\n",
        "\n",
        "        # Start off by resetting the team timestep scores\n",
        "        red_team_raw_score_ts = 0\n",
        "        blue_team_raw_score_ts = 0\n",
        "        red_team_acquired_targets_ts = 0\n",
        "        blue_team_stunned_targets_ts = 0\n",
        "\n",
        "        # Iterate through each of our agents/objects needing updating\n",
        "        # We zip these together so one teams move mostly at the same time\n",
        "        for b in zip(l_bots_red, l_bots_blue):\n",
        "        \n",
        "            # Update our bots on each team\n",
        "            update_bot(b[0]) # red bot\n",
        "            update_bot(b[1]) # blue bot\n",
        "\n",
        "        # Move our targets farther away from our bots\n",
        "        for t in l_targets:\n",
        "            move_target(t)\n",
        "\n",
        "        # Since rewards may be affected by other agent actions, only add reward to episode list (used for reinforcement) after all actions\n",
        "        # (may or may not actually be used in practice)\n",
        "        for b in l_bots_red:\n",
        "            b.l_episode_rewards.append(b.current_ts_reward)\n",
        "        for b in l_bots_blue:\n",
        "            # blue team is incentivized by red securing targets\n",
        "            red_score_incentive = (red_team_acquired_targets_ts * reward_target) / n_start_bots # apportion it out to all bots\n",
        "            b.current_ts_reward += red_score_incentive\n",
        "            blue_team_raw_score_ts += red_score_incentive\n",
        "            b.l_episode_rewards.append(b.current_ts_reward)\n",
        "\n",
        "        # Increment overall episode scores for tracking\n",
        "        red_team_raw_score_episode += red_team_raw_score_ts\n",
        "        red_team_acquired_targets_episode += red_team_acquired_targets_ts\n",
        "        blue_team_raw_score_episode += blue_team_raw_score_ts\n",
        "        blue_team_stunned_targets_episode += blue_team_stunned_targets_ts\n",
        "\n",
        "        # End of timestep\n",
        "\n",
        "        # Only record playback every n samples\n",
        "        if (i_episode + 1) % n_sample_every == 0:\n",
        "            l_red_obj_scores_playback.append(red_team_acquired_targets_episode)\n",
        "            l_blue_obj_scores_playback.append(blue_team_stunned_targets_episode)\n",
        "            l_red_raw_scores_playback.append(red_team_raw_score_episode)\n",
        "            l_blue_raw_scores_playback.append(blue_team_raw_score_episode)\n",
        "            save_all_a()\n",
        "\n",
        "        # Make sure at least n targets are stil on the grid\n",
        "        replenish_targets()\n",
        "\n",
        "        # Clean up any stuns\n",
        "        clean_stuns()\n",
        "\n",
        "    # End of episode\n",
        "\n",
        "    # Track team scores for this episode\n",
        "    # Raw scores\n",
        "    l_red_raw_scores_episode.append(red_team_raw_score_episode)\n",
        "    l_blue_raw_scores_episode.append(blue_team_raw_score_episode)\n",
        "    l_red_moving_raw_score.append(red_team_raw_score_episode)\n",
        "    l_blue_moving_raw_score.append(blue_team_raw_score_episode)\n",
        "    # Objective scores\n",
        "    l_red_obj_scores_episode.append(red_team_acquired_targets_episode)\n",
        "    l_blue_obj_scores_episode.append(blue_team_stunned_targets_episode)\n",
        "    l_red_moving_obj_score.append(red_team_acquired_targets_episode)\n",
        "    l_blue_moving_obj_score.append(blue_team_stunned_targets_episode)\n",
        "\n",
        "    # Calculate loss and update networks with our reinforcement algorithm\n",
        "    reinforce_botbrain(team_brain_red, l_bots_red, team_optim_red)\n",
        "    reinforce_botbrain(team_brain_blue, l_bots_blue, team_optim_blue)\n",
        "\n",
        "    # Print results\n",
        "    if (i_episode + 1) % 500 == 0:\n",
        "        red_score_text = '  Red team obj/raw/obj-mov/raw-mov score:  ' \\\n",
        "            + f'{red_team_acquired_targets_episode}/' \\\n",
        "            + f'{round(red_team_raw_score_episode, 3)}/' \\\n",
        "            + f'{round(np.mean(l_red_moving_obj_score), 3)}/' \\\n",
        "            + f'{round(np.mean(l_red_moving_raw_score), 3)}.'\n",
        "        blue_score_text = '  Blue team obj/raw/obj-mov/raw-mov score:  ' \\\n",
        "            + f'{blue_team_stunned_targets_episode}/' \\\n",
        "            + f'{round(blue_team_raw_score_episode, 3)}/' \\\n",
        "            + f'{round(np.mean(l_blue_moving_obj_score), 3)}/' \\\n",
        "            + f'{round(np.mean(l_blue_moving_raw_score), 3)}.'\n",
        "        print(f'Finished episode {i_episode + 1}.' + red_score_text + blue_score_text)\n",
        "\n",
        "    # Save a gif of the episode every n episodes\n",
        "    if (i_episode + 1) % n_sample_every == 0:\n",
        "        generate_gif(suffix=str(i_episode + 1), playback_iters=episode_length)\n",
        "        torch.save(team_brain_red.state_dict(), fp_red_brain_statedict + f'_{i_episode + 1}.pth')\n",
        "        torch.save(team_brain_blue.state_dict(), fp_blue_brain_statedict + f'_{i_episode + 1}.pth')\n",
        "        torch.save(team_optim_red.state_dict(), fp_red_optim_statedict + f'_{i_episode + 1}.pth')\n",
        "        torch.save(team_optim_blue.state_dict(), fp_blue_optim_statedict + f'_{i_episode + 1}.pth')\n",
        "\n",
        "    # Reset anything needed in the environment\n",
        "    reset_environment()\n",
        "\n",
        "print('Done with simultation.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5vFDhoXBRj5"
      },
      "outputs": [],
      "source": [
        "a_episode_count = np.arange(1, n_episodes+1)\n",
        "a_episode_lengths = np.array(l_episode_lengths)\n",
        "a_n_targets = np.array(l_n_targets)\n",
        "a_normalized_red_scores = (np.array(l_red_obj_scores_episode) / a_episode_lengths / a_n_targets) / reward_target / n_start_bots\n",
        "a_normalized_blue_scores = (np.array(l_blue_raw_scores_episode) / a_episode_lengths / a_n_targets) / reward_target / n_start_bots\n",
        "np.savetxt('/content/drive/MyDrive/Gridbots_Temp/a_normalized_red_scores.txt', a_normalized_red_scores, delimiter='\\n')\n",
        "np.savetxt('/content/drive/MyDrive/Gridbots_Temp/a_normalized_blue_scores.txt', a_normalized_blue_scores, delimiter='\\n')\n",
        "roll_window = 500\n",
        "a_rolling_red_scores = pd.Series(a_normalized_red_scores).rolling(roll_window).mean().values\n",
        "a_rolling_blue_scores = pd.Series(a_normalized_blue_scores).rolling(roll_window).mean().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb5hrKfvBnEM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "b948df94-3529-4af8-9b80-05e1b7fe647e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "sb.lineplot(x=a_episode_count, y=a_rolling_red_scores, ax=ax, color='red', alpha=0.4)\n",
        "#sb.lineplot(x=a_episode_count, y=a_rolling_blue_scores, ax=ax, color='blue', alpha=0.4)\n",
        "plt.title('Normalized Training Scores \\n (rolling 500 episode average)')\n",
        "plt.savefig('normalized_training_scores.png', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejgH7AK3_u9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}